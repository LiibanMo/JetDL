# autograd

## gradient metadata

- (py) requires_grad
  - (py) bool
  - (C) enum

- (py) grad
  - (py) Tensor
  - (C) Tensor*

- grad_fn
  - (C) struct `Function*`
    - `Tensor*` prev_tensors
    - `size_t` nprev_tensors
    - `Tensor*` current_grad
    - `void (*apply)(const Function*)`

- (py) backward
  - (py) function
  - (C) void function

## Todo

### metadata

[x] add metadata to tensor object
[] write a `utils_metadata_assign_grad` function to assign grad metadata
[] implement `utils_metadata_assign_grad` to all C backend

### backward

[] create autograd subdirectory in:
  [] src
  [] jetdl (as autograd)
[] implement topological sort for backward

- arguments: Tensor*
- returns `FunctionGraph`
  - FunctionGraph contains:
    - `Function* grad_fns`
    - `size_t ngrad_fns`
[] implement traversal through graph and apply gradients
- arguments: FunctionGraph*
- returns: void
- what it does:
  - calls `grad_fn->apply(grad_fn)` to get gradient
  - `apply` calculates the gradients of previous tensors
[] implement the full `backward` function in C:
- does topological sort to output `FunctionGraph*`
- takes traverses through `FunctionGraph*` to apply gradients
[] expose `backward` as a method for `Tensor` in Python

### Clean-up

[] Move all `typedef` objects in `utils/typedefs.h`
